#!/usr/bin/env python
"""
build_slim_blip2_manual.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â— ëª©ì 
  BLIP-2 OPT-2.7B ì²´í¬í¬ì¸íŠ¸ì—ì„œ Q-Former(+ì¡°ê¸ˆì˜ ViT)ë§Œ ê°€ì ¸ì™€,
  â€¢ Vision : OpenCLIP ViT-L/14 (hidden 1024)
  â€¢ LLM    : OPT-1.3B
  ë¡œ ìƒˆë¡œ â€œìŠ¬ë¦¼(â‰ˆ1.73 B)â€ ëª¨ë¸ì„ ì¡°ë¦½í•´ ì €ì¥í•œë‹¤.

â— ì „ì œ
  blip2_qformer_only/
      â”œâ”€ model-00001-of-00002.safetensors   (Q-Former + ViT ì¼ë¶€)
      â”œâ”€ model.safetensors.index.json
      â”œâ”€ config.json, generation_config.json
      â”œâ”€ preprocessor_config.json
      â””â”€ tokenizer.json â€¦

â— ì„¤ì¹˜
  pip install torch safetensors transformers open-clip-torch==2.24.0 timm

â— ê²°ê³¼
  blip2_vitl14_opt13b_slim/
      â”œâ”€ config.json, generation_config.json, preprocessor_config.json â€¦
      â”œâ”€ model-00001-of-00002.safetensors   (8-bit ì–‘ìí™”)
      â””â”€ tokenizer.*  â†’ 1.73 B íŒŒë¼ë¯¸í„°
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports
import os, torch, safetensors.torch as sf
from transformers import (
    Blip2Config, Blip2ForConditionalGeneration, Blip2Processor,
    OPTForCausalLM, CLIPVisionConfig, BitsAndBytesConfig
)
from open_clip import create_model_and_transforms

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ paths
SRC_DIR  = "blip2_qformer_only"
SHARD    = os.path.join(SRC_DIR, "model-00001-of-00002.safetensors")
DEST_DIR = "blip2_vitl14_opt13b_slim"
os.makedirs(DEST_DIR, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. Load small LLM (OPT-1.3B)
print("ğŸ”„  Loading OPT-1.3B â€¦")
lm_small = OPTForCausalLM.from_pretrained(
    "facebook/opt-1.3b",
    torch_dtype=torch.float16,
    use_safetensors=True,       # .safetensorsë§Œ
    low_cpu_mem_usage=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Load ViT-L/14 backbone
print("ğŸ”„  Loading OpenCLIP ViT-L/14 â€¦")
vit_l14, _, _ = create_model_and_transforms("ViT-L-14", "openai")
vision_model  = vit_l14.visual
vision_cfg    = CLIPVisionConfig.from_pretrained("openai/clip-vit-large-patch14")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Build empty BLIP-2 skeleton
print("ğŸ› ï¸   Building BLIP-2 skeleton â€¦")
cfg = Blip2Config(
    vision_config  = vision_cfg.to_dict(),          # dict í˜•íƒœ í•„ìš”
    qformer_config = {"hidden_size": 768, "num_hidden_layers": 12},
    text_config    = lm_small.config.to_dict(),
)
model = Blip2ForConditionalGeneration(cfg)
model.vision_model   = vision_model
model.language_model = lm_small
model.lm_proj        = torch.nn.Linear(768, lm_small.config.hidden_size, bias=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. Inject Q-Former & ViT weights
print("ğŸ”„  Injecting pretrained Q-Former & ViT weights â€¦")
state = sf.load_file(SHARD, device="cpu")

def copy_submodule(prefix: str, target):
    """
    prefix: 'qformer.' or 'vision_model.'
    weight shapeê°€ ì¼ì¹˜í•˜ëŠ” í•­ëª©ë§Œ ë³µì‚¬, ë‚˜ë¨¸ì§€ëŠ” ì´ˆê¸°í™” ìœ ì§€
    """
    src = {k[len(prefix):]: v for k, v in state.items() if k.startswith(prefix)}
    tgt_sd = target.state_dict()
    filtered = {k: v for k, v in src.items() if k in tgt_sd and v.shape == tgt_sd[k].shape}
    skipped  = [k for k in src if k not in filtered]
    tgt_sd.update(filtered)
    target.load_state_dict(tgt_sd, strict=False)
    print(f"    â€¢ {prefix[:-1]:<12}  copied {len(filtered):>4}  |  skipped {len(skipped):>2} (shape mismatch)")

copy_submodule("qformer.",      model.qformer)      # cross-attn key/value(1408â†’1024)ëŠ” skip
copy_submodule("vision_model.", model.vision_model)
print("âœ…  Weight injection done.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Freeze everything except Q-Former & lm_proj
for n, p in model.named_parameters():
    if not n.startswith(("qformer", "lm_proj")):
        p.requires_grad_(False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. Save processor + 8-bit model
print("ğŸ’¾  Saving processor & 8-bit model â€¦")
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
processor.save_pretrained(DEST_DIR)

bnb_cfg = BitsAndBytesConfig(load_in_8bit=True)
model.save_pretrained(DEST_DIR, safe_serialization=True, quantization_config=bnb_cfg)

param_B = sum(p.numel() for p in model.parameters()) / 1e9
print(f"ğŸ‰  Done!  Slim checkpoint â†’ {DEST_DIR}/   (params â‰ˆ {param_B:.2f} B)")
