'''
inference_final_corrected.py

í›ˆë ¨ëœ ëª¨ë¸ í´ë˜ìŠ¤(BlipForQuestionAnswering)ì™€ ì¼ì¹˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ ,
ìƒì„±ëœ ë‹µë³€ì„ ì„ íƒì§€ì™€ ë¹„êµí•˜ì—¬ ìµœì¢… ë‹µì„ ê²°ì •í•˜ëŠ” ìµœì¢… ìŠ¤í¬ë¦½íŠ¸.
'''
import torch
from transformers import (
    BlipForQuestionAnswering, # ğŸ“Œ 1. í›ˆë ¨ ì‹œ ì‚¬ìš©í–ˆë˜ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¡œ ë³€ê²½
    BlipProcessor
)
from PIL import Image
import pandas as pd
import os
from tqdm import tqdm
import sys

# --- 1. ì„¤ì • (Configuration) ---
class CONFIG:
    # ğŸ“Œ 2. ì›ë³¸ ëª¨ë¸ IDì™€ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ëª…í™•íˆ ë¶„ë¦¬
    BASE_MODEL_ID = "Salesforce/blip-vqa-base"
    FINETUNED_MODEL_PATH = "./stage1_checkpoints/checkpoint-500" 
    
    TEST_CSV_PATH = "open/test.csv"
    SUBMISSION_CSV_PATH = "submission.csv"
    IMAGE_BASE_PATH = "open"

def load_model_and_processor(model_path, base_model_id):
    """
    í”„ë¡œì„¸ì„œëŠ” ì›ë³¸ IDì—ì„œ, ëª¨ë¸ì€ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"í”„ë¡œì„¸ì„œ ë¡œë”©: {base_model_id}")
    processor = BlipProcessor.from_pretrained(base_model_id)

    print(f"í•™ìŠµëœ ëª¨ë¸ ë¡œë”©: {model_path}")
    model = BlipForQuestionAnswering.from_pretrained(model_path, device_map="auto")
    model.eval()
    return model, processor

# ğŸ“Œ 3. ì˜ˆì¸¡ í•¨ìˆ˜ ë¡œì§ì„ 'ë‹µë³€ ìƒì„± í›„ ë¹„êµ' ë°©ì‹ìœ¼ë¡œ ë³€ê²½
def predict_vqa_and_choose_option(
    image_path, question, options,
    model, processor
):
    """
    ëª¨ë¸ì´ ììœ  ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•œ ë’¤, ì„ íƒì§€ì™€ ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ì˜µì…˜ì„ ì„ íƒí•©ë‹ˆë‹¤.
    """
    try:
        image = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        return "A" # ê¸°ë³¸ê°’

    device = model.device
    
    inputs = processor(images=image, text=question, return_tensors="pt").to(device)

    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=10, num_beams=3)
    
    generated_answer = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

    # ìƒì„±ëœ ë‹µë³€ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì„ íƒì§€ ì°¾ê¸° (ë‹¨ì–´ ê²¹ì¹¨ ê¸°ì¤€)
    best_option = "A" 
    max_overlap = -1

    for key, value in options.items():
        if generated_answer.lower() == str(value).lower():
            return key

    for key, value in options.items():
        option_words = set(str(value).lower().split())
        answer_words = set(generated_answer.lower().split())
        overlap = len(option_words.intersection(answer_words))
        
        if overlap > max_overlap:
            max_overlap = overlap
            best_option = key
            
    return best_option

def main():
    model, processor = load_model_and_processor(
        model_path=CONFIG.FINETUNED_MODEL_PATH,
        base_model_id=CONFIG.BASE_MODEL_ID
    )
    
    df_test = pd.read_csv(CONFIG.TEST_CSV_PATH)
    results = []

    print("ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc="Processing"):
        img_id = row['ID']
        img_path = os.path.join(CONFIG.IMAGE_BASE_PATH, row['img_path'].lstrip('./'))
        question = row['Question']
        options = {
            'A': row['A'], 'B': row['B'],
            'C': row['C'], 'D': row['D']
        }

        answer = predict_vqa_and_choose_option(
            image_path=img_path,
            question=question,
            options=options,
            model=model,
            processor=processor
        )

        results.append({'ID': img_id, 'answer': answer})

    print("ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    results_df = pd.DataFrame(results)
    results_df.to_csv(CONFIG.SUBMISSION_CSV_PATH, index=False)
    print(f"ì„±ê³µì ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±: {CONFIG.SUBMISSION_CSV_PATH}")

if __name__ == "__main__":
    main()