'''
train_model_final.py

train.csvì˜ ê°ê´€ì‹ í˜•ì‹ê³¼ vitual7w.csvì˜ ì„œìˆ í˜• ì •ë‹µ í˜•ì‹ì„
ëª¨ë‘ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ VQADataset í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•œ ìµœì¢… ë²„ì „.
'''
import torch
from transformers import (
    BlipForQuestionAnswering,
    BlipProcessor,
    TrainingArguments,
    Trainer
)
from PIL import Image
import pandas as pd
import os

# --- 1. ì„¤ì • (Configuration) ---
class CONFIG:
    SEED = 42
    BASE_MODEL_ID = "Salesforce/blip-vqa-base"
    
    STAGE1_DATA_PATH = "open/vitual7w.csv"
    STAGE1_MODEL_SAVE_PATH = "./model_stage1_vitual7w"

    STAGE2_DATA_PATH = "open/train.csv"
    FINAL_MODEL_SAVE_PATH = "./final_vqa_model"

    IMAGE_BASE_PATH = "open"
    
    STAGE1_TRAINING_ARGS = {
        "output_dir": "./stage1_checkpoints",
        "seed": SEED,
        "num_train_epochs": 1,
        "learning_rate": 3e-5,
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "save_strategy": "steps",
        "save_steps": 500,
        "eval_strategy": "steps",
        "eval_steps": 500,
        "load_best_model_at_end": True,
        "save_total_limit": 2,
        "logging_steps": 50,
        "remove_unused_columns": False,
        "fp16": True,
    }
    STAGE2_TRAINING_ARGS = {
        "output_dir": "./stage2_checkpoints",
        "seed": SEED,
        "num_train_epochs": 5,
        "learning_rate": 2e-5,
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "save_strategy": "epoch",
        "eval_strategy": "epoch",
        "load_best_model_at_end": True,
        "save_total_limit": 2,
        "logging_steps": 20,
        "remove_unused_columns": False,
        "fp16": True,
    }

# --- 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ---
class VQADataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, processor, image_base_path):
        self.df = dataframe
        self.processor = processor
        self.image_base_path = image_base_path

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(self.image_base_path, str(row['img_path']).lstrip('./'))
        
        try:
            image = Image.open(img_path).convert("RGB")
        except FileNotFoundError:
            image = Image.new('RGB', (self.processor.image_processor.size['height'], self.processor.image_processor.size['width']))
            
        question = str(row['Question'])
        
        # ğŸ“Œ train.csv í˜•ì‹(A,B,C,D ì»¬ëŸ¼ ì¡´ì¬)ê³¼ vitual7w í˜•ì‹ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë¡œì§
        if 'A' in row and 'answer' in row and row['answer'] in ['A', 'B', 'C', 'D']:
            # train.csv í˜•ì‹: answer ì»¬ëŸ¼ì˜ ì•ŒíŒŒë²³ìœ¼ë¡œ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ì˜´
            correct_option_letter = str(row['answer']).strip().upper()
            answer_text = str(row[correct_option_letter])
        else:
            # vitual7w.csv í˜•ì‹: answer ì»¬ëŸ¼ì— ì´ë¯¸ ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ìˆìŒ
            answer_text = str(row['answer'])
        
        encoding = self.processor(images=image, text=question, padding="max_length", truncation=True, return_tensors="pt")
        labels = self.processor.tokenizer(answer_text, padding="max_length", truncation=True, return_tensors="pt").input_ids

        encoding = {k: v.squeeze() for k, v in encoding.items()}
        encoding["labels"] = labels.squeeze()
        return encoding

# --- 3. íŒŒë¼ë¯¸í„° ì¶œë ¥ í•¨ìˆ˜ ---
def print_trainable_parameters(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_param = sum(p.numel() for p in model.parameters())
    print(
        f"ì´ íŒŒë¼ë¯¸í„°: {all_param/1e6:.2f}M | "
        f"í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params/1e6:.2f}M | "
        f"í›ˆë ¨ ê°€ëŠ¥ ë¹„ìœ¨: {100 * trainable_params / all_param:.2f}%"
    )

# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    processor = BlipProcessor.from_pretrained(CONFIG.BASE_MODEL_ID)
    
    # --- 1ë‹¨ê³„: ì¤‘ê°„ í•™ìŠµ ---
    print("--- 1ë‹¨ê³„ ì‹œì‘: ì¤‘ê°„ í•™ìŠµ ---")
    model_s1 = BlipForQuestionAnswering.from_pretrained(CONFIG.BASE_MODEL_ID)
    print_trainable_parameters(model_s1)

    df_s1 = pd.read_csv(CONFIG.STAGE1_DATA_PATH)
    train_df_s1 = df_s1.sample(frac=0.9, random_state=CONFIG.SEED)
    eval_df_s1 = df_s1.drop(train_df_s1.index)
    
    train_dataset_s1 = VQADataset(train_df_s1, processor, CONFIG.IMAGE_BASE_PATH)
    eval_dataset_s1 = VQADataset(eval_df_s1, processor, CONFIG.IMAGE_BASE_PATH)

    training_args_s1 = TrainingArguments(**CONFIG.STAGE1_TRAINING_ARGS)
    trainer_s1 = Trainer(
        model=model_s1,
        args=training_args_s1,
        train_dataset=train_dataset_s1,
        eval_dataset=eval_dataset_s1,
    )
    trainer_s1.train()
    trainer_s1.save_model(CONFIG.STAGE1_MODEL_SAVE_PATH)
    processor.save_pretrained(CONFIG.STAGE1_MODEL_SAVE_PATH)
    del model_s1, trainer_s1, df_s1, train_df_s1, eval_df_s1
    torch.cuda.empty_cache()

    # --- 2ë‹¨ê³„: ìµœì¢… ë¯¸ì„¸ ì¡°ì • ---
    print("\n--- 2ë‹¨ê³„ ì‹œì‘: ìµœì¢… ë¯¸ì„¸ ì¡°ì • ---")
    model_s2 = BlipForQuestionAnswering.from_pretrained(CONFIG.STAGE1_MODEL_SAVE_PATH)
    print_trainable_parameters(model_s2)

    df_s2 = pd.read_csv(CONFIG.STAGE2_DATA_PATH)
    train_df_s2 = df_s2.sample(frac=0.9, random_state=CONFIG.SEED)
    eval_df_s2 = df_s2.drop(train_df_s2.index)

    train_dataset_s2 = VQADataset(train_df_s2, processor, CONFIG.IMAGE_BASE_PATH)
    eval_dataset_s2 = VQADataset(eval_df_s2, processor, CONFIG.IMAGE_BASE_PATH)
    
    training_args_s2 = TrainingArguments(**CONFIG.STAGE2_TRAINING_ARGS)
    trainer_s2 = Trainer(
        model=model_s2,
        args=training_args_s2,
        train_dataset=train_dataset_s2,
        eval_dataset=eval_dataset_s2,
    )
    trainer_s2.train()
    trainer_s2.save_model(CONFIG.FINAL_MODEL_SAVE_PATH)
    processor.save_pretrained(CONFIG.FINAL_MODEL_SAVE_PATH)
    
    print("\n--- 2ë‹¨ê³„ ë¯¸ì„¸ ì¡°ì • ì™„ë£Œ! ---")
    print(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {CONFIG.FINAL_MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()